{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transformer model with pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim                         # Adam optimizer\n",
    "import torch.nn.functional as F                     # Softmax function\n",
    "from torch.utils.data import DataLoader, Dataset    # Loading batches\n",
    "import torch.nn.utils.rnn as rnn_utils              # Padding the sequence\n",
    "from torch.optim.lr_scheduler import OneCycleLR                  # Learning rate scheduler\n",
    "from transformers import AutoTokenizer              # BPE Tokenizer\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30522"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# BPE TOKENIZER TEST\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "# get number of tokens\n",
    "tokenizer.vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['P', 'S', 'E', ' ', '!', '\"', '#', '$', '%', '&', \"'\", '*', '+', ',', '-', '.', '/', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ':', ';', '=', '?', '[', ']', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', '£']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "60"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_slogans = pd.read_csv('all_slogans.csv', sep=';')\n",
    "slogans = all_slogans['slogan']\n",
    "slogans = slogans.str.lower()\n",
    "\n",
    "# reducing invaluable tokens\n",
    "to_remove = ['\\n', '\\r', '>', '\\x80', '\\x93', '\\x94', '\\x99', '\\x9d', '\\xa0',\n",
    "             '¦', '®', '°', 'º', '¼', '½','×', 'â', 'ã', 'è', 'é', 'ï', 'ñ', 'ú', 'ü',\n",
    "             '⁄', '（', '）', '，', '·']\n",
    "\n",
    "dict_to_remove = {\"’\" : \"'\", \"‘\" : \"'\", \"“\" : '\"', \"”\" : '\"',\n",
    "                  \"…\" : '...', '—': '-', '–': '-'}\n",
    "\n",
    "\n",
    "# removing useless toknes\n",
    "for char in to_remove:\n",
    "    slogans = slogans.str.replace(char, ' ')\n",
    "\n",
    "# replacing tokens with normalised versions\n",
    "for key, value in dict_to_remove.items():\n",
    "    slogans = slogans.str.replace(key, value)\n",
    "\n",
    "\n",
    "# getting the characters (tokens) set\n",
    "characters = [char for slogan in slogans for char in slogan]\n",
    "characters = sorted((set(characters)))\n",
    "\n",
    "\n",
    "# adding in the end of every slogan 'E' end token\n",
    "slogans = slogans + 'E'\n",
    "characters = ['E'] + characters\n",
    "\n",
    "# adding the start of sequence token 'S'\n",
    "slogans = slogans.apply(lambda x: 'S' + x)\n",
    "characters = ['S'] + characters\n",
    "\n",
    "# Add padding token at 0 index\n",
    "characters = ['P'] + characters\n",
    "\n",
    "\n",
    "print(characters)\n",
    "len(characters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoding string to integers sequence\n",
    "# decoding integers to string sequence\n",
    "to_int = {char: idx for idx, char in enumerate(characters)}\n",
    "to_str = {idx: char for idx, char in enumerate(characters)}\n",
    "\n",
    "encode = lambda sentence: [to_int[char] for char in sentence]\n",
    "decode = lambda sentence: [to_str[char] for char in sentence]\n",
    "\n",
    "encoded_slogans = [encode(slogan) for slogan in slogans]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define hyperparameters\n",
    "vocab_size = len(characters)\n",
    "d_model = 32 # dim of the embedding vector               # TO CHANGE\n",
    "nhead = 8 # number of attention heads\n",
    "num_decoder_layers = 3 # number of decoder layers\n",
    "dim_feedforward = 2048 # feed-forward network dimension\n",
    "max_seq_length = 100 \n",
    "batch_size = 128\n",
    "dropout = 0.1\n",
    "PAD_TOKEN = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([128, 99]) torch.Size([128, 99])\n",
      "Sa noble scotch - blended for connoisseurs.PPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPP\n",
      "torch.Size([128, 99, 32])\n"
     ]
    }
   ],
   "source": [
    "class SloganDataset(Dataset):\n",
    "    def __init__(self, slogans, encode, max_seq_length=100):\n",
    "        self.slogans = slogans\n",
    "        self.encode = encode\n",
    "        self.max_seq_length = max_seq_length\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.slogans)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        slogan = self.slogans[idx]\n",
    "        \n",
    "        # Truncate if slogan is too long\n",
    "        if len(slogan) > self.max_seq_length:\n",
    "            slogan = slogan[:self.max_seq_length]     \n",
    "\n",
    "        input_sequence = torch.tensor(self.encode(slogan[:-1]), dtype=torch.long)\n",
    "        target_sequence = torch.tensor(self.encode(slogan[1:]), dtype=torch.long)\n",
    "        return input_sequence, target_sequence\n",
    "    \n",
    "\n",
    "# padding the sequence (For the largest in batch)\n",
    "def collate_fn(batch): \n",
    "    input_sequences, target_sequences = zip(*batch)\n",
    "    input_sequences_padded = rnn_utils.pad_sequence(input_sequences, batch_first=True, padding_value=0)\n",
    "    target_sequences_padded = rnn_utils.pad_sequence(target_sequences, batch_first=True, padding_value=0)\n",
    "    return input_sequences_padded, target_sequences_padded\n",
    "\n",
    "\n",
    "# Test with subset of slogans\n",
    "subset_slogans = slogans\n",
    "dataset = SloganDataset(subset_slogans, encode)\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "\n",
    "# Example of batch\n",
    "for batch in dataloader:\n",
    "    input_sequences, target_sequences = batch\n",
    "    print(input_sequences.shape, target_sequences.shape)\n",
    "    print(''.join(decode(input_sequences[0].tolist())))\n",
    "    print(nn.Embedding(vocab_size, d_model)(input_sequences).shape)\n",
    "    break\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building Positional Encoding and masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., -inf, -inf, -inf, -inf],\n",
       "        [0., 0., -inf, -inf, -inf],\n",
       "        [0., 0., 0., -inf, -inf],\n",
       "        [0., 0., 0., 0., -inf],\n",
       "        [0., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sinusoidal positional encoding\n",
    "def positional_encoding(seq_len, embed_dim):\n",
    "    pe = torch.zeros(seq_len, embed_dim)\n",
    "    for pos in range(seq_len):\n",
    "        for i in range(0, embed_dim, 2):\n",
    "            pe[pos, i] = math.sin(pos / (10000 ** (2 * i / embed_dim)))\n",
    "            pe[pos, i + 1] = math.cos(pos / (10000 ** (2 * i / embed_dim)))\n",
    "    return pe # Watch the change\n",
    "\n",
    "# Generate padding mask to prevent looking at not used tokens\n",
    "def generate_padding_mask(sequence, pad_token = 0):\n",
    "    return (sequence != pad_token).unsqueeze(1).unsqueeze(2)\n",
    "\n",
    "# Generate look ahead mask to prevent looking at future tokens\n",
    "def generate_look_ahead_mask(size):\n",
    "    mask = torch.triu(torch.ones(size, size) * float('-inf'), diagonal=1)\n",
    "    return mask\n",
    "\n",
    "generate_look_ahead_mask(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 100, 32])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 100, 32])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test\n",
    "print(nn.Parameter(torch.zeros(1, max_seq_length, d_model)).shape)\n",
    "positional_encoding(max_seq_length, d_model).unsqueeze(0). shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerModel(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model, nhead, num_decoder_layers, \n",
    "                 dim_feedforward, max_seq_length):\n",
    "        super(TransformerModel, self).__init__()\n",
    "        \n",
    "        # Create the token embedding\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
    "\n",
    "        # Initialize weights with Xavier normal for stability\n",
    "        nn.init.xavier_normal_(self.embedding.weight) \n",
    "\n",
    "        # Unsqueeze to add batch dimension\n",
    "        self.pos_encoder = positional_encoding(max_seq_length, d_model).unsqueeze(0).to(device)\n",
    "\n",
    "        # Transformer Decoder layers\n",
    "        self.transformer_decoder_layer = nn.TransformerDecoderLayer(\n",
    "            d_model=d_model, nhead=nhead, dim_feedforward=dim_feedforward, batch_first=True\n",
    "        )\n",
    "        self.transformer_decoder = nn.TransformerDecoder(\n",
    "            self.transformer_decoder_layer, num_layers=num_decoder_layers\n",
    "        )\n",
    "\n",
    "        self.fc_out = nn.Linear(d_model, vocab_size)\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "\n",
    "    def forward(self, src):\n",
    "        tgt_mask = generate_look_ahead_mask(src.size(1)).to(src.device) # check the change to 1\n",
    "        # sqrt for stabilization\n",
    "        src = self.embedding(src) * math.sqrt(d_model) # (batch_size, seq_len, d_model)\n",
    "        # add positional encoding \n",
    "        src = src + self.pos_encoder[:, :src.size(1), :] # src.size(1) = seq_len\n",
    "        output = self.transformer_decoder(tgt=src, memory=src, tgt_mask=tgt_mask, memory_mask=tgt_mask) # Change the memory mask\n",
    "        output = self.dropout(output)\n",
    "        output = self.fc_out(output)\n",
    "        \n",
    "        return output\n",
    "    \n",
    "model = TransformerModel(vocab_size, d_model, nhead, \n",
    "                          num_decoder_layers, dim_feedforward, max_seq_length).to(device) # Watch out\n",
    "\n",
    "\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=PAD_TOKEN)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.00001)\n",
    "\n",
    "# Warmup with LR scheduling (Cosine annealing)\n",
    "scheduler = OneCycleLR(optimizer, max_lr=0.0001, epochs=20, steps_per_epoch=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0\n",
      "Epoch: 0, Loss: 4.0824761390686035, LR: 0.000007\n",
      "Epoch 1\n",
      "Epoch: 1, Loss: 3.7472176551818848, LR: 0.000017\n",
      "Epoch 2\n",
      "Epoch: 2, Loss: 3.538712501525879, LR: 0.000031\n",
      "Epoch 3\n",
      "Epoch: 3, Loss: 3.3934459686279297, LR: 0.000048\n",
      "Epoch 4\n",
      "Epoch: 4, Loss: 3.2407000064849854, LR: 0.000066\n",
      "Epoch 5\n",
      "Epoch: 5, Loss: 3.1160686016082764, LR: 0.000082\n",
      "Epoch 6\n",
      "Epoch: 6, Loss: 3.0173559188842773, LR: 0.000093\n",
      "Epoch 7\n",
      "Epoch: 7, Loss: 2.903261661529541, LR: 0.000099\n",
      "Epoch 8\n",
      "Epoch: 8, Loss: 2.850273609161377, LR: 0.000100\n",
      "Epoch 9\n",
      "Epoch: 9, Loss: 2.7871315479278564, LR: 0.000098\n",
      "Epoch 10\n",
      "Epoch: 10, Loss: 2.7339131832122803, LR: 0.000096\n",
      "Epoch 11\n",
      "Epoch: 11, Loss: 2.713679790496826, LR: 0.000092\n",
      "Epoch 12\n",
      "Epoch: 12, Loss: 2.6108102798461914, LR: 0.000087\n",
      "Epoch 13\n",
      "Epoch: 13, Loss: 2.599048614501953, LR: 0.000082\n",
      "Epoch 14\n",
      "Epoch: 14, Loss: 2.6029417514801025, LR: 0.000075\n",
      "Epoch 15\n",
      "Epoch: 15, Loss: 2.607839822769165, LR: 0.000068\n",
      "Epoch 16\n",
      "Epoch: 16, Loss: 2.5925872325897217, LR: 0.000060\n",
      "Epoch 17\n",
      "Epoch: 17, Loss: 2.5095252990722656, LR: 0.000052\n",
      "Epoch 18\n",
      "Epoch: 18, Loss: 2.5755558013916016, LR: 0.000044\n",
      "Epoch 19\n",
      "Epoch: 19, Loss: 2.5396690368652344, LR: 0.000036\n"
     ]
    }
   ],
   "source": [
    "# Example training loop with dataloader\n",
    "num_epochs = 20\n",
    "for epoch in range(num_epochs):\n",
    "    print(f'Epoch {epoch}')\n",
    "    for batch in dataloader:\n",
    "        # Move to GPU\n",
    "        input_sequences, target_sequences = batch\n",
    "        input_sequences = input_sequences.to(device)\n",
    "        target_sequences = target_sequences.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(input_sequences)\n",
    "        loss = criterion(output.view(-1, vocab_size), target_sequences.view(-1))\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "        \n",
    "    print(f'Epoch: {epoch}, Loss: {loss.item()}, LR: {scheduler.get_last_lr()[0]:.6f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated slogan: Sthe the the the the the the the the the the the the t the the the the the the t the the t t the t t\n"
     ]
    }
   ],
   "source": [
    "def generate_slogan(model, start_sequence, max_lenght=100):\n",
    "    model.eval()\n",
    "    input_sequence = torch.tensor(encode(start_sequence), dtype=torch.long).unsqueeze(0)\n",
    "    generated_sequence = input_sequence.tolist()[0]\n",
    "\n",
    "    for _ in range(max_lenght - len(start_sequence)):   # Watch out\n",
    "        input_tensor = torch.tensor(generated_sequence[-max_lenght:], dtype=torch.long).unsqueeze(0).to(device)\n",
    "        with torch.no_grad():\n",
    "            output = model(input_tensor)\n",
    "        next_token = torch.argmax(F.softmax(output[0, -1, :], dim=0)).item()\n",
    "        generated_sequence.append(next_token)\n",
    "        if to_str[next_token] == 'E':\n",
    "            break\n",
    "    \n",
    "    return ''.join([to_str[idx] for idx in generated_sequence])\n",
    "\n",
    "start_sequence = \"S\"\n",
    "generated_slogan = generate_slogan(model, start_sequence)\n",
    "print(f\"Generated slogan: {generated_slogan}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cuda_pytorch_dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
