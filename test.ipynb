{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transformer model with pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' ', '!', '\"', '#', '$', '%', '&', \"'\", '*', '+', ',', '-', '.', '/', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ':', ';', '=', '?', '[', ']', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', '£']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "57"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_slogans = pd.read_csv('all_slogans.csv', sep=';')\n",
    "slogans = all_slogans['slogan']\n",
    "slogans = slogans.str.lower()\n",
    "\n",
    "# reducing invaluable tokens\n",
    "to_remove = ['\\n', '\\r', '>', '\\x80', '\\x93', '\\x94', '\\x99', '\\x9d', '\\xa0',\n",
    "             '¦', '®', '°', 'º', '¼', '½','×', 'â', 'ã', 'è', 'é', 'ï', 'ñ', 'ú', 'ü',\n",
    "             '⁄', '（', '）', '，', '·']\n",
    "\n",
    "dict_to_remove = {\"’\" : \"'\", \"‘\" : \"'\", \"“\" : '\"', \"”\" : '\"',\n",
    "                  \"…\" : '...', '—': '-', '–': '-'}\n",
    "\n",
    "\n",
    "# normalizing the tokens\n",
    "for char in to_remove:\n",
    "    slogans = slogans.str.replace(char, ' ')\n",
    "\n",
    "\n",
    "for key, value in dict_to_remove.items():\n",
    "    slogans = slogans.str.replace(key, value)\n",
    "\n",
    "\n",
    "# getting the character set\n",
    "\n",
    "characters = [char for slogan in slogans for char in slogan]\n",
    "characters = sorted((set(characters)))\n",
    "print(characters)\n",
    "len(characters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoding string to integers sequence\n",
    "# decoding integers to string sequence\n",
    "to_int = {char: idx for idx, char in enumerate(characters)}\n",
    "to_str = {idx: char for idx, char in enumerate(characters)}\n",
    "\n",
    "encode = lambda sentence: [to_int[char] for char in sentence]\n",
    "decode = lambda sentence: [to_str[char] for char in sentence]\n",
    "\n",
    "encoded_slogans = [torch.tensor(encode(slogan)) for slogan in slogans]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([49, 37, 34,  0, 38, 43, 49, 34, 47, 43, 30, 49, 38, 44, 43, 30, 41,  0,\n",
       "        30, 38, 47, 41, 38, 43, 34,  0, 44, 35,  0, 34, 36, 54, 45, 49])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define hyperparameters\n",
    "vocab_size = len(characters)\n",
    "d_model = 512 # dim of the embedding vector\n",
    "nhead = 8\n",
    "num_encoder_layers = 3\n",
    "num_decoder_layers = 3\n",
    "dim_feedforward = 2048\n",
    "max_seq_length = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Loss: 4.302258491516113\n",
      "Epoch: 0, Loss: 3.8550021648406982\n",
      "Epoch: 0, Loss: 3.3839778900146484\n",
      "Epoch: 0, Loss: 3.747529983520508\n",
      "Epoch: 0, Loss: 3.6151905059814453\n",
      "Epoch: 0, Loss: 3.605558395385742\n",
      "Epoch: 0, Loss: 3.2459630966186523\n",
      "Epoch: 0, Loss: 3.5550854206085205\n",
      "Epoch: 0, Loss: 2.7653021812438965\n",
      "Epoch: 0, Loss: 2.939154863357544\n",
      "Epoch: 0, Loss: 3.381559133529663\n",
      "Epoch: 0, Loss: 3.327558755874634\n",
      "Epoch: 0, Loss: 3.8296079635620117\n",
      "Epoch: 0, Loss: 3.983919620513916\n",
      "Epoch: 0, Loss: 3.4186267852783203\n",
      "Epoch: 0, Loss: 3.138753890991211\n",
      "Epoch: 0, Loss: 3.4627184867858887\n",
      "Epoch: 0, Loss: 3.1635944843292236\n",
      "Epoch: 0, Loss: 3.2358314990997314\n",
      "Epoch: 0, Loss: 3.0759270191192627\n",
      "Epoch: 1, Loss: 3.041801691055298\n",
      "Epoch: 1, Loss: 2.9392406940460205\n",
      "Epoch: 1, Loss: 2.958998203277588\n",
      "Epoch: 1, Loss: 3.1173813343048096\n",
      "Epoch: 1, Loss: 2.901092290878296\n",
      "Epoch: 1, Loss: 3.3675312995910645\n",
      "Epoch: 1, Loss: 2.6880815029144287\n",
      "Epoch: 1, Loss: 3.149712085723877\n",
      "Epoch: 1, Loss: 2.731306791305542\n",
      "Epoch: 1, Loss: 3.028958559036255\n",
      "Epoch: 1, Loss: 3.1815185546875\n",
      "Epoch: 1, Loss: 3.1212680339813232\n",
      "Epoch: 1, Loss: 3.2521231174468994\n",
      "Epoch: 1, Loss: 3.2910847663879395\n",
      "Epoch: 1, Loss: 3.179993152618408\n",
      "Epoch: 1, Loss: 3.1105291843414307\n",
      "Epoch: 1, Loss: 3.1538283824920654\n",
      "Epoch: 1, Loss: 3.0953965187072754\n",
      "Epoch: 1, Loss: 3.065044403076172\n",
      "Epoch: 1, Loss: 3.0283560752868652\n",
      "Epoch: 2, Loss: 2.922320604324341\n",
      "Epoch: 2, Loss: 2.843059539794922\n",
      "Epoch: 2, Loss: 2.7789063453674316\n",
      "Epoch: 2, Loss: 2.9206860065460205\n",
      "Epoch: 2, Loss: 2.914982318878174\n",
      "Epoch: 2, Loss: 3.4439151287078857\n",
      "Epoch: 2, Loss: 2.765786647796631\n",
      "Epoch: 2, Loss: 3.115689992904663\n",
      "Epoch: 2, Loss: 2.77154278755188\n",
      "Epoch: 2, Loss: 2.9777443408966064\n",
      "Epoch: 2, Loss: 3.181487798690796\n",
      "Epoch: 2, Loss: 2.879772186279297\n",
      "Epoch: 2, Loss: 3.2939987182617188\n",
      "Epoch: 2, Loss: 2.9659762382507324\n",
      "Epoch: 2, Loss: 3.0512478351593018\n",
      "Epoch: 2, Loss: 3.0320205688476562\n",
      "Epoch: 2, Loss: 3.1103169918060303\n",
      "Epoch: 2, Loss: 2.9907596111297607\n",
      "Epoch: 2, Loss: 3.079483985900879\n",
      "Epoch: 2, Loss: 2.826920747756958\n",
      "Epoch: 3, Loss: 2.7308189868927\n",
      "Epoch: 3, Loss: 2.6924972534179688\n",
      "Epoch: 3, Loss: 2.83318829536438\n",
      "Epoch: 3, Loss: 2.7876789569854736\n",
      "Epoch: 3, Loss: 2.7040181159973145\n",
      "Epoch: 3, Loss: 3.2030091285705566\n",
      "Epoch: 3, Loss: 2.769146203994751\n",
      "Epoch: 3, Loss: 3.0938405990600586\n",
      "Epoch: 3, Loss: 2.781688690185547\n",
      "Epoch: 3, Loss: 2.9470858573913574\n",
      "Epoch: 3, Loss: 3.0247323513031006\n",
      "Epoch: 3, Loss: 2.6785695552825928\n",
      "Epoch: 3, Loss: 2.979342222213745\n",
      "Epoch: 3, Loss: 3.050348997116089\n",
      "Epoch: 3, Loss: 2.9422659873962402\n",
      "Epoch: 3, Loss: 3.058229684829712\n",
      "Epoch: 3, Loss: 3.064985513687134\n",
      "Epoch: 3, Loss: 2.948604106903076\n",
      "Epoch: 3, Loss: 2.8716821670532227\n",
      "Epoch: 3, Loss: 2.956684112548828\n",
      "Epoch: 4, Loss: 2.869668960571289\n",
      "Epoch: 4, Loss: 2.764526844024658\n",
      "Epoch: 4, Loss: 2.741684675216675\n",
      "Epoch: 4, Loss: 2.847519874572754\n",
      "Epoch: 4, Loss: 2.653200387954712\n",
      "Epoch: 4, Loss: 3.209134817123413\n",
      "Epoch: 4, Loss: 2.512294292449951\n",
      "Epoch: 4, Loss: 2.997709035873413\n",
      "Epoch: 4, Loss: 2.6098222732543945\n",
      "Epoch: 4, Loss: 2.7117862701416016\n",
      "Epoch: 4, Loss: 2.9491584300994873\n",
      "Epoch: 4, Loss: 2.6461002826690674\n",
      "Epoch: 4, Loss: 2.8378102779388428\n",
      "Epoch: 4, Loss: 3.046210765838623\n",
      "Epoch: 4, Loss: 2.936847686767578\n",
      "Epoch: 4, Loss: 3.048977851867676\n",
      "Epoch: 4, Loss: 2.7610273361206055\n",
      "Epoch: 4, Loss: 2.759087085723877\n",
      "Epoch: 4, Loss: 2.966611623764038\n",
      "Epoch: 4, Loss: 2.833112955093384\n"
     ]
    }
   ],
   "source": [
    "class TransformerModel(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model, nhead, num_encoder_layers,\n",
    "                  num_decoder_layers, dim_feedforward, max_seq_length):\n",
    "        super(TransformerModel, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
    "        self.positional_encoding = nn.Parameter(torch.zeros(1, max_seq_length, d_model))\n",
    "        self.transformer = nn.Transformer(d_model, nhead, num_encoder_layers, num_decoder_layers, dim_feedforward)\n",
    "        self.fc_out = nn.Linear(d_model, vocab_size)\n",
    "\n",
    "    def forward(self, src, tgt):\n",
    "        src = self.embedding(src) + self.positional_encoding[:, :src.size(1), :]\n",
    "        tgt = self.embedding(tgt) + self.positional_encoding[:, :tgt.size(1), :]\n",
    "        output = self.transformer(src, tgt)\n",
    "        output = self.fc_out(output)\n",
    "        return output\n",
    "    \n",
    "model = TransformerModel(vocab_size, d_model, nhead, num_encoder_layers,\n",
    "                          num_decoder_layers, dim_feedforward, max_seq_length)\n",
    "\n",
    "\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Example training loop\n",
    "num_epochs = 5\n",
    "for epoch in range(num_epochs):\n",
    "    for slogan in slogans[:20]:\n",
    "        optimizer.zero_grad()\n",
    "        input_sequence = torch.tensor(encode(slogan[:-1]), dtype=torch.long).unsqueeze(0)\n",
    "        target_sequence = torch.tensor(encode(slogan[1:]), dtype=torch.long).unsqueeze(0)\n",
    "        output = model(input_sequence, input_sequence)\n",
    "        loss = criterion(output.view(-1, vocab_size), target_sequence.view(-1))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        print(f'Epoch: {epoch}, Loss: {loss.item()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated slogan: who are yor \n"
     ]
    }
   ],
   "source": [
    "def generate_slogan(model, start_sequence, max_lenght=100):\n",
    "    model.eval()\n",
    "    input_sequence = torch.tensor(encode(start_sequence), dtype=torch.long).unsqueeze(0)\n",
    "    generated_sequence = input_sequence.tolist()[0]\n",
    "\n",
    "    for _ in range(max_lenght - len(start_sequence)):\n",
    "        input_tensor = torch.tensor(generated_sequence[-max_lenght:], dtype=torch.long).unsqueeze(0)\n",
    "        output = model(input_tensor, input_tensor)\n",
    "        next_token = torch.argmax(F.softmax(output[0, -1, :], dim=0)).item()\n",
    "        generated_sequence.append(next_token)\n",
    "        if to_str[next_token] == ' ':\n",
    "            break\n",
    "    \n",
    "    return ''.join([to_str[idx] for idx in generated_sequence])\n",
    "\n",
    "start_sequence = \"who are yo\"\n",
    "generated_slogan = generate_slogan(model, start_sequence)\n",
    "print(f\"Generated slogan: {generated_slogan}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cuda_pytorch_dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
