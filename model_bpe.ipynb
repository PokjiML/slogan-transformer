{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transformer model with pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim                         # Adam optimizer\n",
    "import torch.nn.functional as F                     # Softmax function\n",
    "from torch.utils.data import DataLoader, Dataset    # Loading batches\n",
    "import torch.nn.utils.rnn as rnn_utils              # Padding the sequence\n",
    "from torch.optim.lr_scheduler import OneCycleLR                  # Learning rate scheduler\n",
    "from transformers import AutoTokenizer              # BPE Tokenizer\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_slogans = pd.read_csv('all_slogans.csv', sep=';')\n",
    "slogans = all_slogans['slogan']\n",
    "slogans = slogans.str.lower()\n",
    "\n",
    "# reducing invaluable tokens\n",
    "to_remove = ['\\n', '\\r', '>', '\\x80', '\\x93', '\\x94', '\\x99', '\\x9d', '\\xa0',\n",
    "             '¦', '®', '°', 'º', '¼', '½','×', 'â', 'ã', 'è', 'é', 'ï', 'ñ', 'ú', 'ü',\n",
    "             '⁄', '（', '）', '，', '·']\n",
    "\n",
    "dict_to_remove = {\"’\" : \"'\", \"‘\" : \"'\", \"“\" : '\"', \"”\" : '\"',\n",
    "                  \"…\" : '...', '—': '-', '–': '-'}\n",
    "\n",
    "\n",
    "# removing useless toknes\n",
    "for char in to_remove:\n",
    "    slogans = slogans.str.replace(char, ' ')\n",
    "\n",
    "# replacing tokens with normalised versions\n",
    "for key, value in dict_to_remove.items():\n",
    "    slogans = slogans.str.replace(key, value)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[CLS] the way i like to travel [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# BPE tokenizer for bert\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "# Tokenizing the dataset\n",
    "encoded_slogans = tokenizer.batch_encode_plus(\n",
    "    slogans.tolist(),\n",
    "    add_special_tokens=True, # <BoS> and <EoS>\n",
    "    padding=True,            # Pad for same seq_length\n",
    "    truncation=True,         # Truncate to max length\n",
    "    return_tensors='pt'      # Torch datatype\n",
    ")\n",
    "\n",
    "# Focusing only on tokens\n",
    "encoded_slogans = encoded_slogans['input_ids']\n",
    "\n",
    "# test example\n",
    "encoded_slogans.shape\n",
    "tokenizer.decode(encoded_slogans[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define hyperparameters\n",
    "vocab_size = tokenizer.vocab_size\n",
    "d_model = 384 # dim of the embedding vector               # TO CHANGE\n",
    "nhead = 8 # number of attention heads\n",
    "num_decoder_layers = 3 # number of decoder layers\n",
    "dim_feedforward = 2048 # feed-forward network dimension\n",
    "max_seq_length = 20                                       # TO CHANGE \n",
    "batch_size = 128\n",
    "dropout = 0.1\n",
    "PAD_TOKEN = tokenizer.pad_token_id\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SloganDataset(Dataset):\n",
    "    def __init__(self, encoded_slogans, max_seq_length=20):\n",
    "        self.encoded_slogans = encoded_slogans ### CHANGE TO ENCODED SLOGANS\n",
    "        self.max_seq_length = max_seq_length\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.encoded_slogans)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        slogan = self.encoded_slogans[idx]\n",
    "        \n",
    "        # Truncate if slogan is too long\n",
    "        if len(slogan) > self.max_seq_length:\n",
    "            slogan = slogan[:self.max_seq_length]     \n",
    "\n",
    "        input_sequence = slogan[:-1]\n",
    "        target_sequence = slogan[1:]\n",
    "        return input_sequence, target_sequence\n",
    "    \n",
    "\n",
    "\n",
    "# Test with subset of slogans\n",
    "subset_encoded_slogans = encoded_slogans\n",
    "dataset = SloganDataset(subset_encoded_slogans)\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building Positional Encoding and masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sinusoidal positional encoding\n",
    "def positional_encoding(seq_len, embed_dim):\n",
    "    pe = torch.zeros(seq_len, embed_dim)\n",
    "    for pos in range(seq_len):\n",
    "        for i in range(0, embed_dim, 2):\n",
    "            pe[pos, i] = math.sin(pos / (10000 ** (2 * i / embed_dim)))\n",
    "            pe[pos, i + 1] = math.cos(pos / (10000 ** (2 * i / embed_dim)))\n",
    "    return pe.unsqueeze(0) # Output for batch_dim propagation\n",
    "\n",
    "# Generate padding mask to prevent looking at not used tokens\n",
    "def generate_padding_mask(sequence, pad_token=0):\n",
    "    mask = (sequence == pad_token).float()\n",
    "    mask = mask.masked_fill(mask == 1, float('-inf')).masked_fill(mask == 0, float(0.0))\n",
    "    return mask\n",
    "\n",
    "# Generate look ahead mask to prevent looking at future tokens\n",
    "def generate_look_ahead_mask(size):\n",
    "    mask = torch.triu(torch.ones(size, size) * float('-inf'), diagonal=1)\n",
    "    return mask\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerModel(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model, nhead, num_decoder_layers, \n",
    "                 dim_feedforward, max_seq_length):\n",
    "        super(TransformerModel, self).__init__()\n",
    "        \n",
    "        # Create the token embedding\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
    "\n",
    "        # Initialize weights with Xavier normal for stability\n",
    "        nn.init.xavier_normal_(self.embedding.weight) \n",
    "\n",
    "        # Unsqueeze to add batch dimension\n",
    "        self.pos_encoder = positional_encoding(max_seq_length, d_model).to(device)\n",
    "\n",
    "        # Transformer Decoder layers\n",
    "        self.transformer_decoder_layer = nn.TransformerDecoderLayer(\n",
    "            d_model=d_model, nhead=nhead, dim_feedforward=dim_feedforward, batch_first=True)\n",
    "        self.transformer_decoder = nn.TransformerDecoder(\n",
    "            self.transformer_decoder_layer, num_layers=num_decoder_layers\n",
    "        )\n",
    "\n",
    "        self.fc_out = nn.Linear(d_model, vocab_size)\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "\n",
    "    def forward(self, src):\n",
    "        # Generate look ahead mask to prevent looking at future tokens\n",
    "        tgt_mask = generate_look_ahead_mask(src.size(1)).to(device) # check the change to 1\n",
    "        # Use padding mask to prevent looking at not used tokens\n",
    "        src_pad_mask = generate_padding_mask(src).to(device)\n",
    "        # sqrt for stabilization\n",
    "        src = self.embedding(src) * math.sqrt(d_model) # (batch_size, seq_len, d_model)\n",
    "        # add positional encoding \n",
    "        src = src + self.pos_encoder[:, :src.size(1), :] # src.size(1) = seq_len\n",
    "        output = self.transformer_decoder(tgt=src, memory=src, tgt_mask=tgt_mask,\n",
    "                                          memory_mask=tgt_mask, tgt_key_padding_mask=src_pad_mask) # Change the memory mask\n",
    "        output = self.dropout(output)\n",
    "        output = self.fc_out(output)\n",
    "        \n",
    "        return output\n",
    "    \n",
    "model = TransformerModel(vocab_size, d_model, nhead, \n",
    "                          num_decoder_layers, dim_feedforward, max_seq_length).to(device) # Watch out\n",
    "\n",
    "\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=PAD_TOKEN)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0005)\n",
    "\n",
    "# Warmup with LR scheduling (Cosine annealing)\n",
    "scheduler = OneCycleLR(optimizer, max_lr=0.0001, epochs=20, steps_per_epoch=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0\n",
      "Epoch: 0, Loss: 8.952378273010254, LR: 0.000007\n",
      "Epoch 1\n",
      "Epoch: 1, Loss: 8.133126258850098, LR: 0.000017\n",
      "Epoch 2\n",
      "Epoch: 2, Loss: 7.200924396514893, LR: 0.000031\n",
      "Epoch 3\n",
      "Epoch: 3, Loss: 6.444535255432129, LR: 0.000048\n",
      "Epoch 4\n",
      "Epoch: 4, Loss: 5.773065090179443, LR: 0.000066\n",
      "Epoch 5\n",
      "Epoch: 5, Loss: 5.479922771453857, LR: 0.000082\n",
      "Epoch 6\n",
      "Epoch: 6, Loss: 5.400160789489746, LR: 0.000093\n",
      "Epoch 7\n",
      "Epoch: 7, Loss: 4.892806053161621, LR: 0.000099\n",
      "Epoch 8\n",
      "Epoch: 8, Loss: 4.753689289093018, LR: 0.000100\n",
      "Epoch 9\n",
      "Epoch: 9, Loss: 4.50779390335083, LR: 0.000098\n",
      "Epoch 10\n",
      "Epoch: 10, Loss: 4.216386318206787, LR: 0.000096\n",
      "Epoch 11\n",
      "Epoch: 11, Loss: 4.13455867767334, LR: 0.000092\n",
      "Epoch 12\n",
      "Epoch: 12, Loss: 3.9435667991638184, LR: 0.000087\n",
      "Epoch 13\n",
      "Epoch: 13, Loss: 3.761566162109375, LR: 0.000082\n",
      "Epoch 14\n",
      "Epoch: 14, Loss: 3.7107436656951904, LR: 0.000075\n",
      "Epoch 15\n",
      "Epoch: 15, Loss: 3.506303071975708, LR: 0.000068\n",
      "Epoch 16\n",
      "Epoch: 16, Loss: 3.4615390300750732, LR: 0.000060\n",
      "Epoch 17\n",
      "Epoch: 17, Loss: 3.30448579788208, LR: 0.000052\n",
      "Epoch 18\n",
      "Epoch: 18, Loss: 3.268707752227783, LR: 0.000044\n",
      "Epoch 19\n",
      "Epoch: 19, Loss: 3.1325066089630127, LR: 0.000036\n"
     ]
    }
   ],
   "source": [
    "# Example training loop with dataloader\n",
    "num_epochs = 20\n",
    "for epoch in range(num_epochs):\n",
    "    print(f'Epoch {epoch}')\n",
    "    for batch in dataloader:\n",
    "        # Move to GPU\n",
    "        input_sequences, target_sequences = batch\n",
    "        input_sequences = input_sequences.to(device)   # To GPU\n",
    "        target_sequences = target_sequences.to(device) # To GPU\n",
    "        optimizer.zero_grad()\n",
    "        output = model(input_sequences)\n",
    "        loss = criterion(output.view(-1, vocab_size), target_sequences.view(-1))\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "        \n",
    "    print(f'Epoch: {epoch}, Loss: {loss.item()}, LR: {scheduler.get_last_lr()[0]:.6f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated slogan:  better  . better than the best . \n"
     ]
    }
   ],
   "source": [
    "def generate_slogan(model, start_sequence, max_lenght=20):\n",
    "    model.eval()\n",
    "    input_sequence = torch.tensor(tokenizer.encode(start_sequence), dtype=torch.long).unsqueeze(0)\n",
    "    generated_sequence = input_sequence.tolist()[0]\n",
    "\n",
    "    for _ in range(max_lenght - len(start_sequence)):   # Watch out\n",
    "        input_tensor = torch.tensor(generated_sequence[-max_lenght:], dtype=torch.long).unsqueeze(0).to(device)\n",
    "        with torch.no_grad():\n",
    "            output = model(input_tensor)\n",
    "        next_token = torch.argmax(F.softmax(output[0, -1, :], dim=0)).item()\n",
    "        generated_sequence.append(next_token)\n",
    "        if next_token == 102:\n",
    "            break\n",
    "    \n",
    "    return ' '.join([tokenizer.decode(idx, skip_special_tokens=True) for idx in generated_sequence])\n",
    "\n",
    "start_sequence = \"better\"\n",
    "generated_slogan = generate_slogan(model, start_sequence)\n",
    "print(f\"Generated slogan: {generated_slogan}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cuda_pytorch_dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
