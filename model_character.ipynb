{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transformer model with pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim                         # Adam optimizer\n",
    "import torch.nn.functional as F                     # Softmax function\n",
    "from torch.utils.data import DataLoader, Dataset    # Loading batches\n",
    "import torch.nn.utils.rnn as rnn_utils              # Padding the sequence\n",
    "from torch.optim.lr_scheduler import OneCycleLR                  # Learning rate scheduler\n",
    "from transformers import AutoTokenizer              # BPE Tokenizer\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'slogans' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 6\u001b[0m\n\u001b[1;32m      2\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m AutoTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbert-base-uncased\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# get number of tokens\u001b[39;00m\n\u001b[1;32m      5\u001b[0m encoding \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mencode_plus(\n\u001b[0;32m----> 6\u001b[0m     \u001b[43mslogans\u001b[49m[\u001b[38;5;241m0\u001b[39m],\n\u001b[1;32m      7\u001b[0m     max_length\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m,\n\u001b[1;32m      8\u001b[0m     padding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmax_length\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      9\u001b[0m     truncation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m     10\u001b[0m     return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     11\u001b[0m )\n\u001b[1;32m     13\u001b[0m tokenizer\u001b[38;5;241m.\u001b[39mdecode(encoding[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;241m0\u001b[39m])\n",
      "\u001b[0;31mNameError\u001b[0m: name 'slogans' is not defined"
     ]
    }
   ],
   "source": [
    "# BPE TOKENIZER TEST\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "# get number of tokens\n",
    "encoding = tokenizer.encode_plus(\n",
    "    slogans[0],\n",
    "    max_length=10,\n",
    "    padding=\"max_length\",\n",
    "    truncation=True,\n",
    "    return_tensors=\"pt\"\n",
    ")\n",
    "\n",
    "tokenizer.decode(encoding['input_ids'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['P', 'S', 'E', ' ', '!', '\"', '#', '$', '%', '&', \"'\", '*', '+', ',', '-', '.', '/', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ':', ';', '=', '?', '[', ']', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', '£']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "60"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_slogans = pd.read_csv('all_slogans.csv', sep=';')\n",
    "slogans = all_slogans['slogan']\n",
    "slogans = slogans.str.lower()\n",
    "\n",
    "# reducing invaluable tokens\n",
    "to_remove = ['\\n', '\\r', '>', '\\x80', '\\x93', '\\x94', '\\x99', '\\x9d', '\\xa0',\n",
    "             '¦', '®', '°', 'º', '¼', '½','×', 'â', 'ã', 'è', 'é', 'ï', 'ñ', 'ú', 'ü',\n",
    "             '⁄', '（', '）', '，', '·']\n",
    "\n",
    "dict_to_remove = {\"’\" : \"'\", \"‘\" : \"'\", \"“\" : '\"', \"”\" : '\"',\n",
    "                  \"…\" : '...', '—': '-', '–': '-'}\n",
    "\n",
    "\n",
    "# removing useless toknes\n",
    "for char in to_remove:\n",
    "    slogans = slogans.str.replace(char, ' ')\n",
    "\n",
    "# replacing tokens with normalised versions\n",
    "for key, value in dict_to_remove.items():\n",
    "    slogans = slogans.str.replace(key, value)\n",
    "\n",
    "\n",
    "# getting the characters (tokens) set\n",
    "characters = [char for slogan in slogans for char in slogan]\n",
    "characters = sorted((set(characters)))\n",
    "\n",
    "\n",
    "# adding in the end of every slogan 'E' end token\n",
    "slogans = slogans + 'E'\n",
    "characters = ['E'] + characters\n",
    "\n",
    "# adding the start of sequence token 'S'\n",
    "slogans = slogans.apply(lambda x: 'S' + x)\n",
    "characters = ['S'] + characters\n",
    "\n",
    "# Add padding token at 0 index\n",
    "characters = ['P'] + characters\n",
    "\n",
    "\n",
    "print(characters)\n",
    "len(characters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoding string to integers sequence\n",
    "# decoding integers to string sequence\n",
    "to_int = {char: idx for idx, char in enumerate(characters)}\n",
    "to_str = {idx: char for idx, char in enumerate(characters)}\n",
    "\n",
    "encode = lambda sentence: [to_int[char] for char in sentence]\n",
    "decode = lambda sentence: [to_str[char] for char in sentence]\n",
    "\n",
    "encoded_slogans = [encode(slogan) for slogan in slogans]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define hyperparameters\n",
    "vocab_size = len(characters)\n",
    "d_model = 32 # dim of the embedding vector               # TO CHANGE\n",
    "nhead = 8 # number of attention heads\n",
    "num_decoder_layers = 3 # number of decoder layers\n",
    "dim_feedforward = 2048 # feed-forward network dimension\n",
    "max_seq_length = 100 \n",
    "batch_size = 128\n",
    "dropout = 0.1\n",
    "PAD_TOKEN = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SloganDataset(Dataset):\n",
    "    def __init__(self, slogans, encode, max_seq_length=100):\n",
    "        self.slogans = slogans\n",
    "        self.encode = encode\n",
    "        self.max_seq_length = max_seq_length\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.slogans)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        slogan = self.slogans[idx]\n",
    "        \n",
    "        # Truncate if slogan is too long\n",
    "        if len(slogan) > self.max_seq_length:\n",
    "            slogan = slogan[:self.max_seq_length]     \n",
    "\n",
    "        input_sequence = torch.tensor(self.encode(slogan[:-1]), dtype=torch.long)\n",
    "        target_sequence = torch.tensor(self.encode(slogan[1:]), dtype=torch.long)\n",
    "        return input_sequence, target_sequence\n",
    "    \n",
    "\n",
    "# padding the sequence (For the largest in batch)\n",
    "def collate_fn(batch): \n",
    "    input_sequences, target_sequences = zip(*batch)\n",
    "    input_sequences_padded = rnn_utils.pad_sequence(input_sequences, batch_first=True, padding_value=0)\n",
    "    target_sequences_padded = rnn_utils.pad_sequence(target_sequences, batch_first=True, padding_value=0)\n",
    "    return input_sequences_padded, target_sequences_padded\n",
    "\n",
    "\n",
    "# Test with subset of slogans\n",
    "subset_slogans = slogans\n",
    "dataset = SloganDataset(subset_slogans, encode)\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building Positional Encoding and masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sinusoidal positional encoding\n",
    "def positional_encoding(seq_len, embed_dim):\n",
    "    pe = torch.zeros(seq_len, embed_dim)\n",
    "    for pos in range(seq_len):\n",
    "        for i in range(0, embed_dim, 2):\n",
    "            pe[pos, i] = math.sin(pos / (10000 ** (2 * i / embed_dim)))\n",
    "            pe[pos, i + 1] = math.cos(pos / (10000 ** (2 * i / embed_dim)))\n",
    "    return pe # Watch the change\n",
    "\n",
    "# Generate padding mask to prevent looking at not used tokens\n",
    "def generate_padding_mask(sequence, pad_token=0):\n",
    "    mask = (sequence == pad_token).float()\n",
    "    mask = mask.masked_fill(mask == 1, float('-inf')).masked_fill(mask == 0, float(0.0))\n",
    "    return mask\n",
    "\n",
    "# Generate look ahead mask to prevent looking at future tokens\n",
    "def generate_look_ahead_mask(size):\n",
    "    mask = torch.triu(torch.ones(size, size) * float('-inf'), diagonal=1)\n",
    "    return mask\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf,\n",
       "        -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### TESTING\n",
    "\n",
    "# # Example of batch\n",
    "# for batch in dataloader:\n",
    "#     input_sequences, target_sequences = batch\n",
    "#     src = input_sequences\n",
    "#     break\n",
    "\n",
    "\n",
    "\n",
    "pad_mask = generate_padding_mask(src)\n",
    "pad_mask[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerModel(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model, nhead, num_decoder_layers, \n",
    "                 dim_feedforward, max_seq_length):\n",
    "        super(TransformerModel, self).__init__()\n",
    "        \n",
    "        # Create the token embedding\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
    "\n",
    "        # Initialize weights with Xavier normal for stability\n",
    "        nn.init.xavier_normal_(self.embedding.weight) \n",
    "\n",
    "        # Unsqueeze to add batch dimension\n",
    "        self.pos_encoder = positional_encoding(max_seq_length, d_model).unsqueeze(0).to(device)\n",
    "\n",
    "        # Transformer Decoder layers\n",
    "        self.transformer_decoder_layer = nn.TransformerDecoderLayer(\n",
    "            d_model=d_model, nhead=nhead, dim_feedforward=dim_feedforward, batch_first=True)\n",
    "        self.transformer_decoder = nn.TransformerDecoder(\n",
    "            self.transformer_decoder_layer, num_layers=num_decoder_layers\n",
    "        )\n",
    "\n",
    "        self.fc_out = nn.Linear(d_model, vocab_size)\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "\n",
    "    def forward(self, src):\n",
    "        # Generate look ahead mask to prevent looking at future tokens\n",
    "        tgt_mask = generate_look_ahead_mask(src.size(1)).to(device) # check the change to 1\n",
    "        # Use padding mask to prevent looking at not used tokens\n",
    "        src_pad_mask = generate_padding_mask(src).to(device)\n",
    "        # sqrt for stabilization\n",
    "        src = self.embedding(src) * math.sqrt(d_model) # (batch_size, seq_len, d_model)\n",
    "        # add positional encoding \n",
    "        src = src + self.pos_encoder[:, :src.size(1), :] # src.size(1) = seq_len\n",
    "        output = self.transformer_decoder(tgt=src, memory=src, tgt_mask=tgt_mask,\n",
    "                                          memory_mask=tgt_mask, tgt_key_padding_mask=src_pad_mask) # Change the memory mask\n",
    "        output = self.dropout(output)\n",
    "        output = self.fc_out(output)\n",
    "        \n",
    "        return output\n",
    "    \n",
    "model = TransformerModel(vocab_size, d_model, nhead, \n",
    "                          num_decoder_layers, dim_feedforward, max_seq_length).to(device) # Watch out\n",
    "\n",
    "\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=PAD_TOKEN)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0005)\n",
    "\n",
    "# Warmup with LR scheduling (Cosine annealing)\n",
    "scheduler = OneCycleLR(optimizer, max_lr=0.0001, epochs=20, steps_per_epoch=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0\n",
      "Epoch: 0, Loss: 4.124213695526123, LR: 0.000007\n",
      "Epoch 1\n",
      "Epoch: 1, Loss: 3.791106939315796, LR: 0.000017\n",
      "Epoch 2\n",
      "Epoch: 2, Loss: 3.581543207168579, LR: 0.000031\n",
      "Epoch 3\n",
      "Epoch: 3, Loss: 3.388166904449463, LR: 0.000048\n",
      "Epoch 4\n",
      "Epoch: 4, Loss: 3.2397501468658447, LR: 0.000066\n",
      "Epoch 5\n",
      "Epoch: 5, Loss: 3.1353073120117188, LR: 0.000082\n",
      "Epoch 6\n",
      "Epoch: 6, Loss: 2.985197067260742, LR: 0.000093\n",
      "Epoch 7\n",
      "Epoch: 7, Loss: 2.9241206645965576, LR: 0.000099\n",
      "Epoch 8\n",
      "Epoch: 8, Loss: 2.844715118408203, LR: 0.000100\n",
      "Epoch 9\n",
      "Epoch: 9, Loss: 2.7662816047668457, LR: 0.000098\n",
      "Epoch 10\n",
      "Epoch: 10, Loss: 2.7443525791168213, LR: 0.000096\n",
      "Epoch 11\n",
      "Epoch: 11, Loss: 2.6770524978637695, LR: 0.000092\n",
      "Epoch 12\n",
      "Epoch: 12, Loss: 2.647608757019043, LR: 0.000087\n",
      "Epoch 13\n",
      "Epoch: 13, Loss: 2.6566505432128906, LR: 0.000082\n",
      "Epoch 14\n",
      "Epoch: 14, Loss: 2.5873239040374756, LR: 0.000075\n",
      "Epoch 15\n",
      "Epoch: 15, Loss: 2.5613865852355957, LR: 0.000068\n",
      "Epoch 16\n",
      "Epoch: 16, Loss: 2.5430586338043213, LR: 0.000060\n",
      "Epoch 17\n",
      "Epoch: 17, Loss: 2.5454792976379395, LR: 0.000052\n",
      "Epoch 18\n",
      "Epoch: 18, Loss: 2.601059675216675, LR: 0.000044\n",
      "Epoch 19\n",
      "Epoch: 19, Loss: 2.5464587211608887, LR: 0.000036\n"
     ]
    }
   ],
   "source": [
    "# Example training loop with dataloader\n",
    "num_epochs = 20\n",
    "for epoch in range(num_epochs):\n",
    "    print(f'Epoch {epoch}')\n",
    "    for batch in dataloader:\n",
    "        # Move to GPU\n",
    "        input_sequences, target_sequences = batch\n",
    "        input_sequences = input_sequences.to(device)\n",
    "        target_sequences = target_sequences.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(input_sequences)\n",
    "        loss = criterion(output.view(-1, vocab_size), target_sequences.view(-1))\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "        \n",
    "    print(f'Epoch: {epoch}, Loss: {loss.item()}, LR: {scheduler.get_last_lr()[0]:.6f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated slogan: Syou the the the the the the the the the the the the the the the the theane the the the the the ane \n"
     ]
    }
   ],
   "source": [
    "def generate_slogan(model, start_sequence, max_lenght=100):\n",
    "    model.eval()\n",
    "    input_sequence = torch.tensor(encode(start_sequence), dtype=torch.long).unsqueeze(0)\n",
    "    generated_sequence = input_sequence.tolist()[0]\n",
    "\n",
    "    for _ in range(max_lenght - len(start_sequence)):   # Watch out\n",
    "        input_tensor = torch.tensor(generated_sequence[-max_lenght:], dtype=torch.long).unsqueeze(0).to(device)\n",
    "        with torch.no_grad():\n",
    "            output = model(input_tensor)\n",
    "        next_token = torch.argmax(F.softmax(output[0, -1, :], dim=0)).item()\n",
    "        generated_sequence.append(next_token)\n",
    "        if to_str[next_token] == 'E':\n",
    "            break\n",
    "    \n",
    "    return ''.join([to_str[idx] for idx in generated_sequence])\n",
    "\n",
    "start_sequence = \"Syo\"\n",
    "generated_slogan = generate_slogan(model, start_sequence)\n",
    "print(f\"Generated slogan: {generated_slogan}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cuda_pytorch_dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
